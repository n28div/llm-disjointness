{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VLLM_ATTENTION_BACKEND=FLASHINFER\n"
     ]
    }
   ],
   "source": [
    "%set_env VLLM_ATTENTION_BACKEND=FLASHINFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/mambaforge/envs/llmdisj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-24 20:32:30,748\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the disjointness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/pairs.csv\", header=0, names=[\"c1\", \"c2\", \"disjoint\"])\n",
    "df[\"reason\"] = df.disjoint.apply(lambda x: x if pd.isna(x) else eval(x)[1])\n",
    "df[\"disjoint\"] = df.disjoint.apply(lambda x: x if pd.isna(x) else eval(x)[0])\n",
    "\n",
    "df_inverse = df.copy()\n",
    "df_inverse[\"c1\"] = df[\"c2\"]\n",
    "df_inverse[\"c2\"] = df[\"c1\"]\n",
    "df = pd.concat([df, df_inverse]).reset_index(drop=True)\n",
    "\n",
    "df[\"c1_name\"] = df.c1.str.replace(\"http://dbpedia.org/ontology/\", \"\")\n",
    "df[\"c2_name\"] = df.c2.str.replace(\"http://dbpedia.org/ontology/\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = {\n",
    "    \"naive\": \"Answer only \\\"yes\\\" or \\\"no\\\".\",\n",
    "    \"task_description\": \"This is a question about ontological disjointness, answer only with \\\"yes\\\" or \\\"no\\\"\",\n",
    "    \"few_shot\": \"This is a question about ontological disjointness, answer only with \\\"yes\\\" or \\\"no\\\"\\nExamples of disjoint are: 'person' and 'file system', 'tower' and 'person', 'place' and 'agent', 'continent' and 'sea', 'baseball league' and 'bowling league', 'planet' and 'star'.\\nExamples of not disjoint are: 'basketball player' and 'baseball player', 'means of transportation' and 'reptile', 'garden' and 'historic place', 'president' and 'beauty queen', 'castle' and 'prison'.\",\n",
    "}\n",
    "\n",
    "PROMPT = {\n",
    "    #\"can_a_question\": (\"Can a %s be a %s?\", lambda a: 0 if a == \"yes\" else 1),\n",
    "    \"can_a_question\": (\"Can a %s be a %s?\", lambda a: False if re.match(r\"^\\s*[Yy]es\", a) else True),\n",
    "    #\"are_disjoint\": (\"Is the class %s disjoint from %s?\", lambda a: 1 if a == \"yes\" else 0)\n",
    "    \"are_disjoint\": (\"Is the class %s disjoint from %s?\", lambda a: True if re.match(r\"^\\s*[Yy]es\", a) else False),\n",
    "}\n",
    "\n",
    "LLMS_MAP = {\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\": \"<|start_header_id|>system<|end_header_id|>\\n\\n%s<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n%s\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"google/gemma-2-9b-it\": \"<start_of_turn>user\\n%s\\n%s<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\": \"[INST] %s \\n%s [/INST]\",\n",
    "    \"Qwen/Qwen2-7B-Instruct\": \"{{ if .System }}<|im_start|>system\\n %s<|im_end|>\\n<|im_start|>user\\n%s<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df[df.disjoint.notna()]\n",
    "y = np.array(list(map(int, samples.disjoint)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 20:32:32 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 20:32:33 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:32:33 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:32:33 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:32:33 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:32:34 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "WARNING 07-24 20:32:36 utils.py:562] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "INFO 07-24 20:32:37 model_runner.py:255] Loading model weights took 8.4633 GB\n",
      "INFO 07-24 20:32:39 gpu_executor.py:84] # GPU blocks: 5262, # CPU blocks: 2048\n",
      "INFO 07-24 20:32:40 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-24 20:32:40 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-24 20:32:48 model_runner.py:1117] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2296/2296 [00:20<00:00, 112.62it/s, est. speed input: 3784.44 toks/s, output: 225.24 toks/s]\n",
      "/tmp/ipykernel_27865/587669019.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"prompt\"] = prompts\n",
      "/tmp/ipykernel_27865/587669019.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"output\"] = output\n",
      "/tmp/ipykernel_27865/587669019.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"llm\"] = llm_k\n",
      "/tmp/ipykernel_27865/587669019.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"system_prompt\"] = sp\n",
      "/tmp/ipykernel_27865/587669019.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"prompt\"] = p_k\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:21<00:00, 106.24it/s, est. speed input: 3676.15 toks/s, output: 212.47 toks/s]\n",
      "/tmp/ipykernel_27865/587669019.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"prompt\"] = prompts\n",
      "/tmp/ipykernel_27865/587669019.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"output\"] = output\n",
      "/tmp/ipykernel_27865/587669019.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[\"llm\"] = llm_k\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:30<00:00, 74.49it/s, est. speed input: 3322.58 toks/s, output: 148.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:33<00:00, 69.11it/s, est. speed input: 3151.63 toks/s, output: 138.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [01:51<00:00, 20.50it/s, est. speed input: 3108.49 toks/s, output: 41.01 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [02:00<00:00, 18.99it/s, est. speed input: 2898.23 toks/s, output: 37.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-24 20:38:30 utils.py:562] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 07-24 20:38:30 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-24 20:38:31 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:38:31 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:38:31 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:38:31 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:38:31 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-24 20:38:34 model_runner.py:255] Loading model weights took 9.6313 GB\n",
      "INFO 07-24 20:38:36 gpu_executor.py:84] # GPU blocks: 1678, # CPU blocks: 780\n",
      "INFO 07-24 20:38:37 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-24 20:38:37 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-24 20:38:47 model_runner.py:1117] Graph capturing finished in 10 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2296/2296 [00:27<00:00, 83.77it/s, est. speed input: 2376.01 toks/s, output: 352.10 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:32<00:00, 70.42it/s, est. speed input: 2067.96 toks/s, output: 294.82 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:41<00:00, 54.94it/s, est. speed input: 2162.81 toks/s, output: 264.97 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:42<00:00, 53.53it/s, est. speed input: 2160.64 toks/s, output: 305.20 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [02:23<00:00, 15.97it/s, est. speed input: 2225.86 toks/s, output: 81.93 toks/s]\n",
      "Processed prompts:  16%|█▌        | 371/2296 [00:33<02:09, 14.90it/s, est. speed input: 1537.35 toks/s, output: 60.01 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-24 20:44:13 scheduler.py:1112] Sequence group 12061 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2296/2296 [02:21<00:00, 16.19it/s, est. speed input: 2273.14 toks/s, output: 85.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 20:46:00 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-24 20:46:01 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-24 20:46:03 model_runner.py:255] Loading model weights took 7.0122 GB\n",
      "INFO 07-24 20:46:13 gpu_executor.py:84] # GPU blocks: 4766, # CPU blocks: 2048\n",
      "INFO 07-24 20:46:14 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-24 20:46:14 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-24 20:46:22 model_runner.py:1117] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2296/2296 [00:25<00:00, 90.87it/s, est. speed input: 2376.59 toks/s, output: 1055.40 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:32<00:00, 71.15it/s, est. speed input: 2003.24 toks/s, output: 1067.35 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:36<00:00, 63.05it/s, est. speed input: 2405.72 toks/s, output: 603.52 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:37<00:00, 61.68it/s, est. speed input: 2476.63 toks/s, output: 240.02 toks/s] \n",
      "Processed prompts: 100%|██████████| 2296/2296 [02:17<00:00, 16.69it/s, est. speed input: 2623.55 toks/s, output: 87.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [02:09<00:00, 17.70it/s, est. speed input: 2816.42 toks/s, output: 38.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 20:53:04 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 20:53:04 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:53:04 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:53:05 selector.py:79] Using Flashinfer backend.\n",
      "WARNING 07-24 20:53:05 selector.py:80] Flashinfer will be stuck on llama-2-7b, please avoid using Flashinfer as the backend when running on llama-2-7b.\n",
      "INFO 07-24 20:53:05 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-24 20:53:08 model_runner.py:255] Loading model weights took 8.1381 GB\n",
      "INFO 07-24 20:53:16 gpu_executor.py:84] # GPU blocks: 8714, # CPU blocks: 4681\n",
      "INFO 07-24 20:53:18 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-24 20:53:18 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-24 20:53:26 model_runner.py:1117] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2296/2296 [00:25<00:00, 90.30it/s, est. speed input: 3316.82 toks/s, output: 180.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:27<00:00, 82.57it/s, est. speed input: 3115.59 toks/s, output: 165.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:38<00:00, 60.27it/s, est. speed input: 2876.78 toks/s, output: 120.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [00:36<00:00, 62.71it/s, est. speed input: 3055.83 toks/s, output: 125.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [01:57<00:00, 19.58it/s, est. speed input: 3030.00 toks/s, output: 39.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 2296/2296 [01:54<00:00, 19.99it/s, est. speed input: 3112.68 toks/s, output: 39.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0)\n",
    "results = []\n",
    "\n",
    "for llm_k, llm_template in LLMS_MAP.items():\n",
    "    try:\n",
    "        del llm\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    llm = LLM(model=llm_k, quantization=\"fp8\")\n",
    "\n",
    "    for sp, system_prompt in SYSTEM_PROMPTS.items():\n",
    "        for p_k, (prompt, parse_fn) in PROMPT.items():\n",
    "            prompts = samples.apply(lambda r: prompt % (r.c1_name, r.c2_name), axis=1).to_list()\n",
    "            prompts = [llm_template % (system_prompt, p) for p in prompts]\n",
    "            \n",
    "            outputs = llm.generate(prompts, sampling_params)\n",
    "            output = [o.outputs[0].text for o in outputs]\n",
    "            pred = np.array(list(map(parse_fn, output)))\n",
    "\n",
    "            samples[\"prompt\"] = prompts\n",
    "            samples[\"output\"] = output\n",
    "            samples[\"llm\"] = llm_k\n",
    "            samples[\"system_prompt\"] = sp\n",
    "            samples[\"prompt\"] = p_k\n",
    "            results.append(samples.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results).to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"./results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>QA</th>\n",
       "      <th>LLM</th>\n",
       "      <th>DR</th>\n",
       "      <th>NDF1</th>\n",
       "      <th>F1</th>\n",
       "      <th>SC</th>\n",
       "      <th>mu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.994595</td>\n",
       "      <td>0.257559</td>\n",
       "      <td>0.525339</td>\n",
       "      <td>0.893728</td>\n",
       "      <td>0.667805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.193243</td>\n",
       "      <td>0.629075</td>\n",
       "      <td>0.165797</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.410139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027883</td>\n",
       "      <td>0.491042</td>\n",
       "      <td>0.980836</td>\n",
       "      <td>0.624940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.987309</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.489855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>0.908070</td>\n",
       "      <td>0.686763</td>\n",
       "      <td>0.848432</td>\n",
       "      <td>0.787843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.858819</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.889373</td>\n",
       "      <td>0.847978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.814476</td>\n",
       "      <td>0.677802</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.787660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Naive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.921622</td>\n",
       "      <td>0.795199</td>\n",
       "      <td>0.699129</td>\n",
       "      <td>0.838850</td>\n",
       "      <td>0.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>0.545050</td>\n",
       "      <td>0.861498</td>\n",
       "      <td>0.688308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Positive</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.863514</td>\n",
       "      <td>0.079012</td>\n",
       "      <td>0.445141</td>\n",
       "      <td>0.898084</td>\n",
       "      <td>0.571438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200116</td>\n",
       "      <td>0.516940</td>\n",
       "      <td>0.905052</td>\n",
       "      <td>0.655527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.041892</td>\n",
       "      <td>0.823885</td>\n",
       "      <td>0.050121</td>\n",
       "      <td>0.682056</td>\n",
       "      <td>0.399489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.968919</td>\n",
       "      <td>0.831393</td>\n",
       "      <td>0.752361</td>\n",
       "      <td>0.844948</td>\n",
       "      <td>0.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Negative</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.979730</td>\n",
       "      <td>0.778170</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.898955</td>\n",
       "      <td>0.842785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.757188</td>\n",
       "      <td>0.636318</td>\n",
       "      <td>0.758711</td>\n",
       "      <td>0.750554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Task description</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.956757</td>\n",
       "      <td>0.605735</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.722563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.898649</td>\n",
       "      <td>0.485645</td>\n",
       "      <td>0.540211</td>\n",
       "      <td>0.831010</td>\n",
       "      <td>0.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.297593</td>\n",
       "      <td>0.435858</td>\n",
       "      <td>0.721254</td>\n",
       "      <td>0.554217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.199074</td>\n",
       "      <td>0.495751</td>\n",
       "      <td>0.848432</td>\n",
       "      <td>0.622301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.051351</td>\n",
       "      <td>0.870417</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.736063</td>\n",
       "      <td>0.431198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Gemma 2</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.890553</td>\n",
       "      <td>0.750597</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.825749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Negative</td>\n",
       "      <td>LLama 3</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.537594</td>\n",
       "      <td>0.594947</td>\n",
       "      <td>0.771777</td>\n",
       "      <td>0.722701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Mistral 0.3</td>\n",
       "      <td>0.737838</td>\n",
       "      <td>0.859656</td>\n",
       "      <td>0.654284</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.759591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Few shot</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Qwen 2</td>\n",
       "      <td>0.979730</td>\n",
       "      <td>0.372385</td>\n",
       "      <td>0.544090</td>\n",
       "      <td>0.709930</td>\n",
       "      <td>0.651534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Prompt        QA          LLM        DR      NDF1        F1  \\\n",
       "9              Naive  Positive      Gemma 2  0.994595  0.257559  0.525339   \n",
       "10             Naive  Positive      LLama 3  0.193243  0.629075  0.165797   \n",
       "11             Naive  Positive  Mistral 0.3  1.000000  0.027883  0.491042   \n",
       "8              Naive  Positive       Qwen 2  0.002703  0.987309  0.005122   \n",
       "13             Naive  Negative      Gemma 2  0.708108  0.908070  0.686763   \n",
       "14             Naive  Negative      LLama 3  0.900000  0.858819  0.743719   \n",
       "15             Naive  Negative  Mistral 0.3  0.850000  0.814476  0.677802   \n",
       "12             Naive  Negative       Qwen 2  0.921622  0.795199  0.699129   \n",
       "17  Task description  Positive      Gemma 2  0.993243  0.353439  0.545050   \n",
       "18  Task description  Positive      LLama 3  0.863514  0.079012  0.445141   \n",
       "19  Task description  Positive  Mistral 0.3  1.000000  0.200116  0.516940   \n",
       "16  Task description  Positive       Qwen 2  0.041892  0.823885  0.050121   \n",
       "21  Task description  Negative      Gemma 2  0.968919  0.831393  0.752361   \n",
       "22  Task description  Negative      LLama 3  0.979730  0.778170  0.714286   \n",
       "23  Task description  Negative  Mistral 0.3  0.850000  0.757188  0.636318   \n",
       "20  Task description  Negative       Qwen 2  0.956757  0.605735  0.608247   \n",
       "1           Few shot  Positive      Gemma 2  0.898649  0.485645  0.540211   \n",
       "2           Few shot  Positive      LLama 3  0.762162  0.297593  0.435858   \n",
       "3           Few shot  Positive  Mistral 0.3  0.945946  0.199074  0.495751   \n",
       "0           Few shot  Positive       Qwen 2  0.051351  0.870417  0.066960   \n",
       "5           Few shot  Negative      Gemma 2  0.850000  0.890553  0.750597   \n",
       "6           Few shot  Negative      LLama 3  0.986486  0.537594  0.594947   \n",
       "7           Few shot  Negative  Mistral 0.3  0.737838  0.859656  0.654284   \n",
       "4           Few shot  Negative       Qwen 2  0.979730  0.372385  0.544090   \n",
       "\n",
       "          SC        mu  \n",
       "9   0.893728  0.667805  \n",
       "10  0.652439  0.410139  \n",
       "11  0.980836  0.624940  \n",
       "8   0.964286  0.489855  \n",
       "13  0.848432  0.787843  \n",
       "14  0.889373  0.847978  \n",
       "15  0.808362  0.787660  \n",
       "12  0.838850  0.813700  \n",
       "17  0.861498  0.688308  \n",
       "18  0.898084  0.571438  \n",
       "19  0.905052  0.655527  \n",
       "16  0.682056  0.399489  \n",
       "21  0.844948  0.849405  \n",
       "22  0.898955  0.842785  \n",
       "23  0.758711  0.750554  \n",
       "20  0.719512  0.722563  \n",
       "1   0.831010  0.688879  \n",
       "2   0.721254  0.554217  \n",
       "3   0.848432  0.622301  \n",
       "0   0.736063  0.431198  \n",
       "5   0.811847  0.825749  \n",
       "6   0.771777  0.722701  \n",
       "7   0.786585  0.759591  \n",
       "4   0.709930  0.651534  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agg_results = []\n",
    "for (sys, prompt, llm), data in results.groupby([\"system_prompt\", \"prompt\", \"llm\"]):\n",
    "    # compute disjoint recall\n",
    "    idxs = (data.disjoint == True)\n",
    "    y = data[idxs].disjoint.astype(float).values\n",
    "    pred = data[idxs].output.apply(lambda s: PROMPT[prompt][1](s)).astype(float).values\n",
    "    dr = recall_score(y, pred, zero_division=0)\n",
    "\n",
    "    # compute non-disjoint accuracy\n",
    "    idxs = (data.disjoint == False)\n",
    "    y = 1 - data[idxs].disjoint.astype(float).values\n",
    "    pred = 1 - data[idxs].output.apply(lambda s: PROMPT[prompt][1](s)).astype(float).values\n",
    "    ndf1 = f1_score(y, pred)\n",
    "    \n",
    "    # end2end f1\n",
    "    y = data.disjoint.astype(float).values\n",
    "    pred = data.output.apply(lambda s: PROMPT[prompt][1](s)).astype(float).values\n",
    "    f1 = f1_score(y, pred, zero_division=0)\n",
    "\n",
    "    # symmetry respected\n",
    "    a = data.iloc[:data.shape[0] // 2].output.apply(lambda s: PROMPT[prompt][1](s)).astype(float).values\n",
    "    b = data.iloc[data.shape[0] // 2:].output.apply(lambda s: PROMPT[prompt][1](s)).astype(float).values\n",
    "    sc = (a == b).sum() / a.shape[0]\n",
    "\n",
    "    # compute metrics\n",
    "    agg_results.append({\n",
    "        \"Prompt\": sys,\n",
    "        \"QA\": prompt,\n",
    "        \"LLM\": llm,\n",
    "        \"DR\": dr,\n",
    "        \"NDF1\": ndf1,\n",
    "        \"F1\": f1,\n",
    "        \"SC\": sc\n",
    "    })\n",
    "    \n",
    "aggregated_results = pd.DataFrame(agg_results)\n",
    "aggregated_results[\"mu\"] = aggregated_results.iloc[:, -4:].mean(axis=1)\n",
    "\n",
    "aggregated_results.LLM = aggregated_results.LLM.map({\n",
    "    \"Qwen/Qwen2-7B-Instruct\": \"Qwen 2\",\n",
    "    \"google/gemma-2-9b-it\": \"Gemma 2\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\": \"LLama 3\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\": \"Mistral 0.3\",\n",
    "})\n",
    "\n",
    "aggregated_results.Prompt = aggregated_results.Prompt.map({\"few_shot\": \"Few shot\", \"naive\": \"Naive\", \"task_description\": \"Task description\"})\n",
    "aggregated_results.Prompt = aggregated_results.Prompt.astype(\"category\")\n",
    "aggregated_results.Prompt = aggregated_results.Prompt.cat.set_categories([\"Naive\", \"Task description\", \"Few shot\"])\n",
    "\n",
    "aggregated_results.QA = aggregated_results.QA.map({\"are_disjoint\": \"Positive\", \"can_a_question\": \"Negative\"})\n",
    "aggregated_results.QA = aggregated_results.QA.astype(\"category\")\n",
    "aggregated_results.QA = aggregated_results.QA.cat.set_categories([\"Positive\", \"Negative\"])\n",
    "\n",
    "\n",
    "aggregated_results.sort_values([\"Prompt\", \"QA\", \"LLM\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdisj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
